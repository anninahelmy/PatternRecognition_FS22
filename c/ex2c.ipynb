{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2b\n",
    "\n",
    "We will follow this online tutorial to construct our CNN and train our data: https://www.analyticsvidhya.com/blog/2019/10/building-image-classification-models-cnn-pytorch/\n",
    "\n",
    "First we import the libraries we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# for evaluating the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch libraries and modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "# for importing the images\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the images (you might have to adjust the paths here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (60000, 784); shape of y_train: (60000,).\n",
      "Shape of x_test: (10000, 784); shape of y_test: (10000,).\n"
     ]
    }
   ],
   "source": [
    "# benno\n",
    "train_folder = \"C:/Users/bennx/OneDrive/Desktop/mnist-png-format/train/\"\n",
    "test_folder = \"C:/Users/bennx/OneDrive/Desktop/mnist-png-format/test/\"\n",
    "\n",
    "# david\n",
    "# train_folder = \"../mnist-png-format/train/\"\n",
    "# test_folder = \"../mnist-png-format/test/\"\n",
    "\n",
    "def load_images(folder_path):\n",
    "    data = {float(s): [np.asarray(Image.open(f\"{folder_path}/{s}/{p}\")).reshape(-1, )\n",
    "        for p in listdir(f\"{folder_path}{s}\")\n",
    "    ] for s in listdir(folder_path)}\n",
    "\n",
    "    (ys, xs) = reduce(\n",
    "        lambda a, b: (a[0] + [b[0]]*len(b[1]), a[1] + b[1]),\n",
    "        data.items(),\n",
    "        ([], [])\n",
    "    )\n",
    "    return np.array(ys), np.array(xs)\n",
    "\n",
    "y_test1, x_test1 = load_images(test_folder)\n",
    "y_train1, x_train1 = load_images(train_folder)\n",
    "\n",
    "# normalizing the pixel values (this step helps in optimizing the performance of our model)\n",
    "x_test1 = np.float32((np.true_divide(x_test1, 255.0)))\n",
    "x_train1 = np.float32((np.true_divide(x_train1, 255.0)))\n",
    "\n",
    "print(f\"Shape of x_train: {x_train1.shape}; shape of y_train: {y_train1.shape}.\")\n",
    "print(f\"Shape of x_test: {x_test1.shape}; shape of y_test: {y_test1.shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we work with torch, we have to convert our images into a tensor format.\n",
    "\n",
    "Be aware of dtypes: float64 will be interpreted as double in torch for tensors. int32 will be interpreted as int, but we need torch.LongTensor when we use train()!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: torch.Size([60000, 1, 28, 28]); shape of y_train: torch.Size([60000]).\n",
      "Shape of x_test: torch.Size([10000, 1, 28, 28]); shape of y_test: torch.Size([10000]).\n"
     ]
    }
   ],
   "source": [
    "# converting training images into torch format\n",
    "x_train = x_train1.reshape(60000, 1, 28, 28)\n",
    "x_train  = torch.from_numpy(x_train)\n",
    "\n",
    "# converting test images into torch format\n",
    "x_test = x_test1.reshape(10000, 1, 28, 28)\n",
    "x_test  = torch.from_numpy(x_test)\n",
    "\n",
    "# converting the train target into torch format\n",
    "y_train = y_train1.astype(int);\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_train = y_train.type(torch.LongTensor)\n",
    "\n",
    "# converting the test target into torch format\n",
    "y_test = y_test1.astype(int);\n",
    "y_test = torch.from_numpy(y_test)\n",
    "y_test = y_test.type(torch.LongTensor)\n",
    "\n",
    "# shape of training and test data\n",
    "print(f\"Shape of x_train: {x_train.shape}; shape of y_train: {y_train.shape}.\")\n",
    "print(f\"Shape of x_test: {x_test.shape}; shape of y_test: {y_test.shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let us fill the missing gaps in the exercise CNN template. Since the images above are stored in 28x28 matrices, it is clear that the first gap \"self.expected_input_size\" must have the shape (28, 28). <br>\n",
    "\n",
    "The second gap is a little bit more complicated. Lets skip it and first do the last gap: \"nn.Linear(1536, ___)\". This is the last layer (the layer for the final classification). Our data are digits from 0 to 9, thus the value in the missing gap must be 10, since the result should be one of the 10 classes.<br>\n",
    "\n",
    "Now the second gap. \"in_channels=___\" is also quit simple, our images are in greyscale, so there is only one incoming channel (RGB would be for example 3). We know that the last layer has dimension 1536x1 after flattening. So we can calculate the out_channels and kernel_size, sinze we know the input size and stride. First we do a prime factorization of $1536 = 2^9 \\cdot 3$. Since we have a quadratic matrix, we will have its output size after the first layer squared and then multiplied with the output channels. We can conclude from the prime factorization that the number of out_channels is either 6, 24, 96, ..., therefore we consider the formula to calculate the output size (before out_channels): $o = \\frac{i-k+2p}{s}+1$, where $i$ is the input size, $k$ the kernel size, $p$ the padding and $s$ the stride. In our case $i=28, p=0, s=3$, so to get an reasonable value for $o$ (the kernel should be bigger than 1 and not too big); so we see that if $1 \\leq k \\leq 4$ we will get $o=9$, which cannot be the case, since 1536 has not enough 3's as prime factors. So we conclude that $5 \\leq k \\leq 7$ to get $o=8$; depending on the implementation, but usually the fraction above will be rounded down to the nearest integer, but just to be safe we choose $k=7$. Now we can calculate the missing factors and conclude out_channels = 24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        return x\n",
    "\n",
    "class PR_CNN(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(PR_CNN, self).__init__()\n",
    "\n",
    "        self.expected_input_size = (28, 28)\n",
    "\n",
    "        # First layer\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=24, kernel_size=7, stride=3),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        # Classification layer\n",
    "        self.fc = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(1536, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to define our model. Since we are not really familiar with the best optimizers or loss functions, we just copied the ones from the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR_CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 24, kernel_size=(7, 7), stride=(3, 3))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Flatten()\n",
      "    (1): Linear(in_features=1536, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = PR_CNN()\n",
    "# defining the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.07)\n",
    "# defining the loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have the model of our CNN, but we also need to define a function to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    # getting the training set\n",
    "    x_tr, y_tr = Variable(x_train), Variable(y_train)\n",
    "    # converting the data into GPU format\n",
    "    if torch.cuda.is_available():\n",
    "        x_tr = x_tr.cuda()\n",
    "        y_tr = y_tr.cuda()\n",
    "\n",
    "    # clearing the Gradients of the model parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # prediction for training  set\n",
    "    output_train = model(x_tr)\n",
    "\n",
    "    # computing the training loss\n",
    "    loss_train = criterion(output_train, y_tr)\n",
    "    train_losses.append(loss_train)\n",
    "\n",
    "    # computing the updated weights of all the model parameters\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    if epoch%2 == 0:\n",
    "        # printing the train loss\n",
    "        print('Epoch : ',epoch+1, '\\t', 'loss :', loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we can start train the model. As a test we start with 25 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 \t loss : tensor(0.2301, grad_fn=<NllLossBackward0>)\n",
      "Epoch :  3 \t loss : tensor(0.2139, grad_fn=<NllLossBackward0>)\n",
      "Epoch :  5 \t loss : tensor(0.1974, grad_fn=<NllLossBackward0>)\n",
      "Epoch :  7 \t loss : tensor(0.1838, grad_fn=<NllLossBackward0>)\n",
      "Epoch :  9 \t loss : tensor(0.1696, grad_fn=<NllLossBackward0>)\n",
      "Epoch :  11 \t loss : tensor(0.1591, grad_fn=<NllLossBackward0>)\n",
      "Epoch :  13 \t loss : tensor(0.1479, grad_fn=<NllLossBackward0>)\n",
      "Epoch :  15 \t loss : tensor(0.1401, grad_fn=<NllLossBackward0>)\n",
      "Epoch :  17 \t loss : tensor(0.1311, grad_fn=<NllLossBackward0>)\n",
      "Epoch :  19 \t loss : tensor(0.1253, grad_fn=<NllLossBackward0>)\n",
      "Epoch :  21 \t loss : tensor(0.1187, grad_fn=<NllLossBackward0>)\n",
      "Epoch :  23 \t loss : tensor(0.1138, grad_fn=<NllLossBackward0>)\n",
      "Epoch :  25 \t loss : tensor(0.1089, grad_fn=<NllLossBackward0>)\n",
      "Time spent to the train the model: 36.47 seconds.\n"
     ]
    }
   ],
   "source": [
    "t25 = time.perf_counter()\n",
    "# defining the number of epochs\n",
    "n_epochs = 25\n",
    "# empty list to store training losses\n",
    "train_losses = []\n",
    "# training the model\n",
    "for epoch in range(n_epochs):\n",
    "    train(epoch)\n",
    "t25 = time.perf_counter() - t25\n",
    "print(f\"Time spent to the train the model: {round(t25,2)} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD4CAYAAADrRI2NAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAn4UlEQVR4nO3dd3hVVb7/8fc3jVASWkINvUMogVBDtQ2oYBlQsSBYQEcsOF4H9c6V64x3HHsDFSxYR2woNrDREUwoAqEGCBBaQg0tQJL1+yPRX0SEE0iyk3M+r+fxMWefvff57mz9ZJ+1117LnHOIiEjgCPK6ABERKVkKfhGRAKPgFxEJMAp+EZEAo+AXEQkwIV4XcLKoqCjXsGFDr8sQESlTFi9evNs5F+3LuqUu+Bs2bEhSUpLXZYiIlClmttnXddXUIyISYBT8IiIBRsEvIhJgSl0bv4iUXidOnCAtLY2srCyvSwlY4eHhxMTEEBoaetb7UPCLiM/S0tKIiIigYcOGmJnX5QQc5xx79uwhLS2NRo0anfV+1NQjIj7LysqievXqCn2PmBnVq1c/529cCn4RKRSFvreK4vfvN8F/PDuXf321mm37j3pdiohIqeY3wb/jwFHeW7SFkW8lcfR4jtfliEgx2LNnDx06dKBDhw7UqlWLunXr/vr6+PHjp902KSmJu+6664yf0aNHjyKpddasWVx66aVFsq+i5jc3dxtUr8jzQ+O46c1E/uujn3lhaJy+kor4merVq7Ns2TIAxo0bR6VKlbjvvvt+fT87O5uQkFPHWnx8PPHx8Wf8jAULFhRJraWZ31zxA/RrWYO/9W/JF8t3MGHWBq/LEZESMHz4cG677Ta6du3K/fffz08//UT37t2Ji4ujR48erF27FvjtFfi4ceO46aab6Nu3L40bN+b555//dX+VKlX6df2+ffsyePBgWrZsyXXXXccvMxZ+9dVXtGzZkk6dOnHXXXed8cp+7969XH755bRr145u3bqxfPlyAGbPnv3rN5a4uDgOHjzIjh076N27Nx06dCA2Npa5c+cW+e/Mb674fzGqd2NW78jkyW/W0qJmBBe0rul1SSJ+6X8/T2bV9swi3WfrOpE8PLBNobdLS0tjwYIFBAcHk5mZydy5cwkJCeG7777jwQcf5OOPP/7dNmvWrGHmzJkcPHiQFi1acPvtt/+ub/zSpUtJTk6mTp06JCQkMH/+fOLj4xk1ahRz5syhUaNGDB069Iz1Pfzww8TFxfHpp5/yww8/MGzYMJYtW8aTTz7J+PHjSUhI4NChQ4SHhzNx4kT+9Kc/8dBDD5GTk8ORI0cK/fs4E7+64oe8O97//nM7YutU5p4py1i/66DXJYlIMRsyZAjBwcEAHDhwgCFDhhAbG8uYMWNITk4+5TaXXHIJ5cqVIyoqiho1arBr167frdOlSxdiYmIICgqiQ4cOpKamsmbNGho3bvxrP3pfgn/evHnccMMNAJx33nns2bOHzMxMEhISuPfee3n++efZv38/ISEhdO7cmTfeeINx48axYsUKIiIizvbX8of87oofIDw0mInDOjHwhfnc8lYSn92RQJUKYV6XJeJXzubKvLhUrFjx15///ve/069fP6ZOnUpqaip9+/Y95TblypX79efg4GCys7PPap1zMXbsWC655BK++uorEhISmDFjBr1792bOnDl8+eWXDB8+nHvvvZdhw4YV6ef63RX/L2pXLs8rN3Rkx/4sRr+3lOycXK9LEpEScODAAerWrQvA5MmTi3z/LVq0YOPGjaSmpgIwZcqUM27Tq1cv3n33XSDv3kFUVBSRkZFs2LCBtm3b8re//Y3OnTuzZs0aNm/eTM2aNbn11lu55ZZbWLJkSZEfg98GP0CnBtX45+WxzEvZzb++XuN1OSJSAu6//34eeOAB4uLiivwKHaB8+fJMmDCB/v3706lTJyIiIqhcufJptxk3bhyLFy+mXbt2jB07ljfffBOAZ599ltjYWNq1a0doaCgDBgxg1qxZtG/fnri4OKZMmcLdd99d5Mdgv9ylLi3i4+NdUU/EMm5aMpMXpPLkkPYM7hRTpPsWCSSrV6+mVatWXpfhuUOHDlGpUiWcc9xxxx00a9aMMWPGlNjnn+o8mNli59yZ+6vi51f8v/jvS1qR0LQ6D36ygiVb9nldjoiUcZMmTaJDhw60adOGAwcOMGrUKK9LKpSAuOIH2Hf4OJeNn8/REzl8ProntSqHF/lniPg7XfGXDrri91HVimFMGhbPkWPZjHo7iawTGtZB5GyUtovFQFMUv/+ACX6AFrUieObqDvycdoAHPlmh/4BFCik8PJw9e/bo/x2P/DIef3j4ubVY+GU//tO5qE0t7r2wOU9/u45WtSMY2buJ1yWJlBkxMTGkpaWRkZHhdSkB65cZuM6FT8FvZv2B54Bg4FXn3GMnvX8vcAuQDWQANznnNptZB+AlIBLIAR51zp2502sxu/O8pqzZmcljX6+hec0I+rao4XVJImVCaGjoOc38JKXDGZt6zCwYGA8MAFoDQ82s9UmrLQXinXPtgI+Ax/OXHwGGOefaAP2BZ82sShHVftbMjCeHtKdFrUjufG8pizbu8bokEZES40sbfxcgxTm30Tl3HHgfuKzgCs65mc65X0YSWgjE5C9f55xbn//zdiAdiC6q4s9FhbAQXr0xnujIclz/2iI+WZLmdUkiIiXCl+CvC2wt8Dotf9kfuRn4+uSFZtYFCAN+N16ymY00syQzSyrJtsO6Vcoz9fYE4htU494PfuaZb9fpppWI+L0i7dVjZtcD8cATJy2vDbwNjHDO/W7QHOfcROdcvHMuPjq6ZL8QVK4Qyps3dWFwpxie+349Y6Ys41i2unqKiP/y5ebuNqBegdcx+ct+w8wuAB4C+jjnjhVYHgl8CTzknFt4buUWj7CQIJ4Y3I5GURV5YsZatu/P4pUbOlG1okb0FBH/48sVfyLQzMwamVkYcA0wreAKZhYHvAIMcs6lF1geBkwF3nLOfVR0ZRc9M+OOfk15YWgcy9L2c8WE+WzafdjrskREitwZg985lw2MBmYAq4EPnHPJZvaImQ3KX+0JoBLwoZktM7Nf/jBcBfQGhucvX5bfxbPUGti+Dv+5tSuZWdlcMWG+evyIiN8JmLF6CmvznsOMmJzI1r1HeHxwO66I06ieIlJ6aayeItCgesVfe/yMmaIePyLiPxT8p3Fyj597P/hZPX5EpMwLuLF6CuvkHj/b9h1Vjx8RKdN0xe+Dk3v8/PnlBew4cNTrskREzoqCvxAGtq/DOzd3JSPzGENe/pHNe9TdU0TKHgV/IXVpVI33bu3G4WPZDHn5R9bvOuh1SSIihaLgPwttYyozZVR3HHDVKz+yctsBr0sSEfGZgv8sNa8ZwYejulMhLIShExeSlLrX65JERHyi4D8HDaMq8uFt3YmOKMcNr/3E3PWalUhESj8F/zmqU6U8U0Z1p0H1Ctw8OYlvknd6XZKIyGkp+ItAdEQ53h/ZjdZ1Irn93SV8tux3g5eKiJQaCv4iUqVCGO/c0pXODatyz5RlvLdoi9cliYickoK/CFUqF8LkEV3o2zyaB6euYNKcjV6XJCLyOwr+IhYeGswrN8RzSdvaPPrVag3uJiKljsbqKQZhIUE8PzSOCmHBPPf9eg4fy+ahS1phZl6XJiKi4C8uwUHGv//cjorlQnh13iaCgowHL27ldVkiIgr+4hQUZDw8sDU5uY6JczbSsHpFru1a3+uyRCTAKfiLmVle+G/Ze4S/f7aSetXK06tZtNdliUgA083dEhASHMSL18bRrEYl/vLOEg3sJiKeUvCXkIjwUF4b3pnwsGBGTE4k4+Axr0sSkQCl4C9BdauU59Vh8ew+dIyRbyeRdULTOIpIyVPwl7D29arw7NUdWLZ1P3/98Gdyc9XHX0RKloLfA/1jazO2f0u+XL6Dp75d63U5IhJgfAp+M+tvZmvNLMXMxp7i/XvNbJWZLTez782sQYH3bjSz9fn/3FiUxZdlI3s35prO9Rg/cwMfJm31uhwRCSBnDH4zCwbGAwOA1sBQM2t90mpLgXjnXDvgI+Dx/G2rAQ8DXYEuwMNmVrXoyi+7zIx/XB5Lz6ZRPDh1BT9u2ON1SSISIHy54u8CpDjnNjrnjgPvA5cVXME5N9M5dyT/5UIgJv/nPwHfOuf2Ouf2Ad8C/Yum9LIvNDiI8dd1pEH1itz2zmI2ZBzyuiQRCQC+BH9doGBbRFr+sj9yM/B1YbY1s5FmlmRmSRkZgTWLVeXyobwxvDMhQcZNkxPZe/i41yWJiJ8r0pu7ZnY9EA88UZjtnHMTnXPxzrn46OjAe6q1XrUKTBwWz44DWYx6O4lj2ermKSLFx5fg3wbUK/A6Jn/Zb5jZBcBDwCDn3LHCbCvQqUFVnhrSnsTUfYz9eIWGchaRYuNL8CcCzcyskZmFAdcA0wquYGZxwCvkhX56gbdmABeZWdX8m7oX5S+TUxjYvg73XdScqUu38dz3670uR0T81BkHaXPOZZvZaPICOxh43TmXbGaPAEnOuWnkNe1UAj7MH3N+i3NukHNur5n9g7w/HgCPOOf2FsuR+Ik7+jVl0+4jPPvdeiLDQ7mpZyOvSxIRP2OlrUkhPj7eJSUleV2Gp45n53LXf5YyPXknD13cilt7N/a6JBEp5cxssXMu3pd19eRuKRQWEsQL18b9On3jhFkpXpckIn5E4/GXUqHBQTx3TQeCg4zHp68lJ8dx5/nNvC5LRPyAgr8UCwkO4pmrOxASZDz17Tqycx33XNBMc/eKyDlR8JdywUHGE0PaExxkPPf9erJzc7nvohYKfxE5awr+MuCXidtDgoMYP3MD2TmOsQNaKvxF5Kwo+MuIoCDj0ctjCQkyXpmzkexcx39f0krhLyKFpuAvQ4KCjEcua0NwkPHavE3k5DoeHtha4S8ihaLgL2PMjIcHtiY02Jg0dxMncnL5x2WxBAUp/EXENwr+MsjMePDiVoQEB/HSrA3k5Dr+74q2Cn8R8YmCv4wyM+7/UwtCgowXfkghO9fx7z+3I1jhLyJnoOAvw8yMv17UgpCgIJ75bh1HT+TwxOB2VAjTaRWRP6aE8AN3X9CM8NAgHpu+hvW7DvLS9Z1oEl3J67JEpJTSWD1+YlSfJrx1Uxd2HzrOoBfm8cXy7V6XJCKllILfj/RqFs0Xd/akRa0IRr+3lHHTkjmenet1WSJSyij4/UydKuV5f2R3bkpoxOQFqVz1yo9s23/U67JEpBRR8PuhsJAg/mdgayZc15GU9ENc8vxcZq1NP/OGIhIQFPx+7OK2tZk2OoFakeGMmJzI09+uIye3dE28IyIlT8Hv5xpHV2LqXxK4Mi6G579fz42v/8SeQ8e8LktEPKTgDwDlw4J5ckg7/v3ntvyUupdLnp9HUqqmPhYJVAr+AGFmXN25Pp/c3oOwkCCumbiQV+dupLTNuSwixU/BH2Bi61bm8zt70q9lDf755WrGTFmmLp8iAUbBH4Aqlw9l4g2d+OuFzfl02XZGTP6JzKwTXpclIiVEwR+gzIw7z2/Gk0Pas2jjXq56+Ud2ZWZ5XZaIlAAFf4Ab3CmG14d3ZuveI1wxfj7rdx30uiQRKWY+Bb+Z9TeztWaWYmZjT/F+bzNbYmbZZjb4pPceN7NkM1ttZs+bposqdXo3j2bKqO6cyHX8+aUF/LRJPX5E/NkZg9/MgoHxwACgNTDUzFqftNoWYDjw3knb9gASgHZALNAZ6HPOVUuRi61bmU9u70FURDmuf20RX63Y4XVJIlJMfLni7wKkOOc2OueOA+8DlxVcwTmX6pxbDpzcPcQB4UAYUA4IBXadc9VSLOpVq8DHt/Wgbd3K3PHeEl6ft8nrkkSkGPgS/HWBrQVep+UvOyPn3I/ATGBH/j8znHOrT17PzEaaWZKZJWVkZPiyaykmVSuG8e4tXbmodU0e+WIVj365ilwN8yDiV4r15q6ZNQVaATHk/bE4z8x6nbyec26icy7eORcfHR1dnCWJD8JDg5lwXSdu7N6ASXM3cfeUZRzLzvG6LBEpIr7MwLUNqFfgdUz+Ml9cASx0zh0CMLOvge7A3MIUKSUvOMgYN6gNtauU57Gv15BxMItXboincvlQr0sTkXPkyxV/ItDMzBqZWRhwDTDNx/1vAfqYWYiZhZJ3Y/d3TT1SOpkZt/VpwnPXdGDx5n1c9fKPbNfY/iJl3hmD3zmXDYwGZpAX2h8455LN7BEzGwRgZp3NLA0YArxiZsn5m38EbABWAD8DPzvnPi+G45BidFmHukwe0YXt+49y5YQF6usvUsZZaRukKz4+3iUlJXldhpzC6h2ZDHv9J3JyHW/d1IXYupW9LklE8pnZYudcvC/r6sld8Vmr2pF8OKo75UODGTpxIYka2lmkTFLwS6E0jKrIh7d1JzqiHDe8tojZ69T9VqSsUfBLodWpUp4PbutO46hK3PJmItNX6ilfkbJEwS9nJapSOf4zshtt61bmL+8u4ePFaV6XJCI+UvDLWatcPpS3b+5K9ybV+euHP/PWj6lelyQiPlDwyzmpWC6E127szIWta/I/nyUzfmaK1yWJyBko+OWc5Q3x0JHLO9ThiRlreezrNZrLV6QU82XIBpEzCg0O4umrOlCxXAgvz97AoWMneGRQLEFBmn5BpLRR8EuRCQoy/nl5LJXCQ3hl9kYOH8vhicHtCAnWF0uR0kTBL0XKzBjbvyWR4aE8MWMth49l88K1cZQLCfa6NBHJp0sxKXJmxh39mjJuYGu+WbWLG1//ifSDmshdpLRQ8EuxGZ7QiKevas/SLfu5+Ll5zF2vp3xFSgMFvxSrKzvG8NnoBKpUCGXY6z/x+PQ1ZOecPEOniJQkBb8Uu5a1Ipk2OoEhnWKYMGsDV09cyDaN6y/iGQW/lIgKYSE8Prg9z13TgTU7Mrn4ubnMSN7pdVkiAUnBLyXqsg51+eKuXtSrVp5Rby9m3LRkzecrUsIU/FLiGkVV5OPbezC8R0MmL0jlygkL2LT7sNdliQQMBb94olxIMOMGtWHSsHjS9h3l0ufn8unSbV6XJRIQFPziqQtb1+Tru3vRqnYk90xZxn99+DNHjmd7XZaIX1Pwi+fqVCnP+yO7MbpfUz5aksagF+ezctsBr8sS8VsKfikVQoKDuO9PLXjn5q4cOHqCQS/O45HPV3HomK7+RYqagl9KlYSmUXw3pg9Du9TnjQWbuOCp2Xy1YoeGeRYpQgp+KXUqVwjl0Sva8sntPahWMYy/vLuEEZMT2bLniNelifgFn4LfzPqb2VozSzGzsad4v7eZLTGzbDMbfNJ79c3sGzNbbWarzKxhEdUufi6uflWmjU7g75e2JnHTXi58ZjYv/rBe/f5FztEZg9/MgoHxwACgNTDUzFqftNoWYDjw3il28RbwhHOuFdAFSD+XgiWwhAQHcXPPRnz/176c36oGT36zjgHPzWXBht1elyZSZvlyxd8FSHHObXTOHQfeBy4ruIJzLtU5txz4zehb+X8gQpxz3+avd8g5p+/rUmi1Kocz4bpOvDGiM9k5jmsnLWLMlGVkHDzmdWkiZY4vwV8X2FrgdVr+Ml80B/ab2SdmttTMnsj/BiFyVvq1qME3Y3pz53lN+WL5ds5/ahbvLtpMbq5u/or4qrhv7oYAvYD7gM5AY/KahH7DzEaaWZKZJWVkaMx2Ob3w0GD+elELvr67N23qVOahqSu58qUFpKQf8ro0kTLBl+DfBtQr8Domf5kv0oBl+c1E2cCnQMeTV3LOTXTOxTvn4qOjo33ctQS6pjUq8d6tXXn26g5s3nOYS1+YyzsLN6vrp8gZ+BL8iUAzM2tkZmHANcA0H/efCFQxs1/S/DxgVeHLFDk1M+PyuLrMuKc3nRtW478/XcmtbyWx55Da/kX+yBmDP/9KfTQwA1gNfOCcSzazR8xsEICZdTazNGAI8IqZJedvm0NeM8/3ZrYCMGBS8RyKBLIakeG8OaIL/3Npa+as382fnp3LrLXqQCZyKlbavhbHx8e7pKQkr8uQMmzNzkzu/s8y1u46yPAeDRk7oCXhoepTIP7NzBY75+J9WVdP7orfaVkrks9GJzAiIW+8/8tenM+anZlelyVSaij4xS+Fhwbz8MA2vHlTF/YeOc6gF+bz2rxN6vYpgoJf/Fyf5tFMv7sXvZtH848vVnHjGz+xKzPL67JEPKXgF79XvVI5Jg3rxKNXxJKYupf+z87RRO8S0BT8EhDMjOu6NuCLO3tRt2reRO8PfLJcs31JQFLwS0BpWqMSn9yewKg+jXk/cSsDX5hH8nbN9iWBRcEvAScsJIgHBrTinZu7cjArmyvGL+DVuRt141cChoJfAlZC0yim39Ob3s2j+eeXqxkxOVGjfUpAUPBLQKtWMYxJwzrxj8tjWbhxDwOem8NMPfErfk7BLwHPzLihWwOmje5J9YrlGPFGIv/7ebJm+hK/peAXydeiVgSfjU5geI+GvDE/lcvHLyAl/aDXZYkUOQW/SAHhocGMG9SG126MZ1dmFpe+MI/3Fm3RUM/iVxT8IqdwfquaTL+7F50bVuPBqSu47Z3F7Dt83OuyRIqEgl/kD/wy1PODF7fkhzXpDHhuLjPXpOvqX8o8Bb/IaQQFGSN7N+GT2xOoUC6YEZMTuf61Razcpoe+pOxS8Iv4oG1MZabf3ZuHB7Zm1fZMLn1hHmOmLCNt3xGvSxMpNE3EIlJImVkneGnWBl6ftwnnYHhCQ+7o25TKFUK9Lk0CWGEmYlHwi5yl7fuP8vS36/h4SRqR4aHceV5TbujegHIhmu1LSp5m4BIpAXWqlOfJIe358s5etK9XhX9+uZrzn5rNZ8u2adwfKdUU/CLnqHWdSN66qQtv39yFyPBQ7n5/GZeNn8+CDbu9Lk3klBT8IkWkV7NovrizJ09f1Z49h45x7aRFjHjjJ37eut/r0kR+Q238IsUg60QOkxekMmFmCplZ2fRoUp3b+jShV7MozMzr8sQP6eauSClxMOsE//lpC6/N28SuzGO0qRPJbX2acHHb2gQH6Q+AFB0Fv0gpcyw7h0+XbuOV2RvZuPswDapXYGTvxvy5YwzhoeoFJOeuyHv1mFl/M1trZilmNvYU7/c2syVmlm1mg0/xfqSZpZnZi758noi/KRcSzNWd6/PtvX14+fqOVCkfykNTV9Lz3zOZMCuFzKwTXpcoAeSMV/xmFgysAy4E0oBEYKhzblWBdRoCkcB9wDTn3Ecn7eM5IBrY65wbfbrP0xW/BALnHD9u3MNLszYwd/1uIsqFcG23+tyc0IgakeFelydlUGGu+EN8WKcLkOKc25i/8/eBy4Bfg985l5r/Xu4piukE1ASmAz4VJeLvzIweTaLo0SSKldsO8PLsDUyas5E35qUyJD6Gey5oTnREOa/LFD/lS1NPXWBrgddp+cvOyMyCgKfI+yZwuvVGmlmSmSVlZGT4smsRvxFbtzIvXtuRmff1ZXB8DFMSt9LvyVm8NGsDWSc0C5gUveLux/8X4CvnXNrpVnLOTXTOxTvn4qOjo4u5JJHSqUH1ivzfFW35ZkxvujWuzr+nr+HCZ2YzfeUODQUtRcqX4N8G1CvwOiZ/mS+6A6PNLBV4EhhmZo8VqkKRANM4uhKv3hjP2zd3oUJoCLe9s4RrJi7UUNBSZHwJ/kSgmZk1MrMw4Bpgmi87d85d55yr75xrSF5zz1vOud/1ChKR3+vVLJov7+rJPy+PZX36IQa+OI+/fbSc9INZXpcmZdwZg985lw2MBmYAq4EPnHPJZvaImQ0CMLPOZpYGDAFeMbPk4ixaJFCEBAdxfbcGzLyvLzcnNOLjJWmc9+Rstf/LOdEDXCJlyMaMQ/zfV2v4bvUu6lUrz4MDWtE/tpaGgRANyyzir35p/3/n5q5UCA3h9neXcLXa/6WQFPwiZVDPZlG/tv+n5Lf/P/DJCvYcOuZ1aVIGKPhFyqiC7f8jejTiw6St9H1yFq/N28SJnN89SynyKwW/SBlXuXwo/zOwNdPv6UVc/ar844tV9H92DrPWpntdmpRSCn4RP9G0RgRvjujMazfGk5PrGP5GIjdPTmTT7sNelyaljIJfxI+YGee3qsmMMb15YEBLFm3ay0XPzOZfX63moEYAlXwKfhE/VC4kmFF9mvDDfX24vENdXpmzkX5PzuaDpK2aCF4U/CL+rEZEOE8Mac9ndyRQr1p57v9oOVdMmM+SLfu8Lk08pOAXCQDt61Xh49t68MzV7dmZmcWVExZw3asLmZG8k2z1AAo4enJXJMAcPpbN5AWpvLNwMzsOZFGncjjXdWvA1Z3rEVVJcwCUVZpzV0TOKDsnl+9Wp/P2wlTmp+whLDiIi9vW4obuDelYv4qGgShjinoGLhHxQyHBQfSPrUX/2FqkpB/k7R838/GSbXy6bDtt6kRyY/eGDGxfh/Jhmgze3+iKX0R+dehYNlOXbuPtH1NZt+sQlcuHclV8DNd3a0CD6hW9Lk9OQ009InJOnHMs2rSXt3/czPTkneQ6R8+mUVzarjYXtq5FtYphXpcoJ1Hwi0iR2ZWZxXuLtvDxkjTS9h0lyKBro+oMaFuLi1rXolblcK9LFBT8IlIMnHMkb89k+sqdfL1yBxsy8oaC6Fi/Cv1jazEgtjb1qlXwuMrApeAXkWKXkn6Qr1fsZHryTpK3ZwLQpk4k/dvUYkDbWjStEeFxhYFFwS8iJWrLniPMSM77JrBky34AmkRXZFD7ugyJj6FOlfLeFhgAFPwi4pldmVnMSN7JVyt2sHDjXoIM+jSP5urO9Tm/VQ1CgzVgQHFQ8ItIqbB17xE+SNrKB0lb2ZV5jKhK5RjcKYarO9ejUZS6hxYlBb+IlCrZObnMXpfB+4lb+WFNOjm5jm6Nq3FN5/r0j61FeKgeEjtXCn4RKbV2ZWbx0eI0piRuZcveI1QuH8oVcXW5pks9WtaK9Lq8MkvBLyKlXm6uY+HGPfwncSszVu7keE4u7etV4aaEhlzStjYhuhdQKAp+ESlT9h0+zidLt/Heos1syDhM3SrlubVXI67qXI8KYRpSzBeFCX6f/qSaWX8zW2tmKWY29hTv9zazJWaWbWaDCyzvYGY/mlmymS03s6t9PwwRCRRVK4Zxc89GfDumD68Oi6dOlXDGfb6KHo/9wNPfrmPPoWNel+hXznjFb2bBwDrgQiANSASGOudWFVinIRAJ3AdMc859lL+8OeCcc+vNrA6wGGjlnNv/R5+nK34RAVi8eS8vz97It6t2US4kiKvi63Frr8bUr66ng0+lqIdl7gKkOOc25u/8feAy4Nfgd86l5r/3m6l8nHPrCvy83czSgWhgvy/FiUjg6tSgGpOGVSMl/RCT5mxkSuJW3l20mYvb1mZU7ya0jansdYllli9NPXWBrQVep+UvKxQz6wKEARtO8d5IM0sys6SMjIzC7lpE/FjTGpX49+B2zP1bP0b2bsLstRkMfHEe1726kDnrMiht9ynLghK5bW5mtYG3gRHOud9N8Omcm+ici3fOxUdHR5dESSJSxtSMDGfsgJYseOA8Hry4JSnphxj2+k8MeG4ur83bRMZB3QfwlS/Bvw2oV+B1TP4yn5hZJPAl8JBzbmHhyhMR+a2I8FBG9m7C3PvP44nB7QgNDuIfX6yi27++56bJiXyxfDtZJ3K8LrNU86WNPxFoZmaNyAv8a4Brfdm5mYUBU4G3frnhKyJSFMJCghgSX48h8fVYv+sgnyzdxtQl2/hhTToR4SFc2q42V3aMIb5BVc0ffBKf+vGb2cXAs0Aw8Lpz7lEzewRIcs5NM7PO5AV8VSAL2Omca2Nm1wNvAMkFdjfcObfsjz5LvXpE5Gzl5D8U9vGSNKav3MmR4znUr1aBK+LqcmXHun49faQe4BKRgHf4WDYzknfyyZJtzN+wG+cgvkFVruwYw4DYWlT1s+kjFfwiIgXsOHCUT5du5+MlaaSkH8Isb9KYhCZR9GgaReeGVcv8E8IKfhGRU3DOsXJbJjPXpjM/ZTdLtuzjRI4jNNiIq1+VhCZR9GxWnXYxVcrcvAEKfhERHxw5nk1i6j4WpOxm/obdJG/PxDmoGBZM18bV6dGkOglNo2hZK6LU3yAu6id3RUT8UoWwEPo0j6ZP87znh/YdPs7CjXuYv2E381P28MOadACiKoXRq1k0/VrWoHezKKpUKNv3B3TFLyLyB7bvP8r8lN3MT9nN7HUZ7DtygiCDjvWr0q9lDfq1qEGr2qXj24CaekREilhOruPntP3MWpPOzLUZrNh2AIBakeH0axlN3xY1SGgaRaVy3jSkKPhFRIpZemYWs9ZlMGttOnPX7ebgsWxCg40ujarRr0UNzmtZg8bRlUqsHgW/iEgJOpGTS1LqPmatTWfm2nTW7ToEQMtaEQxsX4eB7eoU+3DSCn4REQ+l7TvCt6t28cXyHSzevA+AdjGVGdiuDpe0q02dKuWL/DMV/CIipcS2/Uf5cvl2vli+g+VpefcF4htU5dJ2tbm4XW1qRIQXyeco+EVESqHU3Yf5csUOPv95O2t2HsQMujWqzqXtazMgtjbVzmEYCQW/iEgpl5J+kM9/3sHny7ezMeMwwUHGgNhavHhtx7Panx7gEhEp5ZrWiGDMhRHcc0EzVu84yOfLtxNUQo8DKPhFRDxkZrSuE0nrOpEl9pllaxQiERE5Zwp+EZEAo+AXEQkwCn4RkQCj4BcRCTAKfhGRAKPgFxEJMAp+EZEAU+qGbDCzDGDzOewiCthdROWUNTr2wBXIxx/Ixw7///gbOOeifdmg1AX/uTKzJF/Hq/A3OvbAPHYI7OMP5GOHszt+NfWIiAQYBb+ISIDxx+Cf6HUBHtKxB65APv5APnY4i+P3uzZ+ERE5PX+84hcRkdNQ8IuIBBi/CX4z629ma80sxczGel1PSTOzVDNbYWbLzMyv5640s9fNLN3MVhZYVs3MvjWz9fn/rupljcXpD45/nJltyz//y8zsYi9rLC5mVs/MZprZKjNLNrO785f7/fk/zbEX+tz7RRu/mQUD64ALgTQgERjqnFvlaWElyMxSgXjnnN8/yGJmvYFDwFvOudj8ZY8De51zj+X/4a/qnPubl3UWlz84/nHAIefck17WVtzMrDZQ2zm3xMwigMXA5cBw/Pz8n+bYr6KQ595frvi7ACnOuY3OuePA+8BlHtckxcQ5NwfYe9Liy4A3839+k7z/IfzSHxx/QHDO7XDOLcn/+SCwGqhLAJz/0xx7oflL8NcFthZ4ncZZ/kLKMAd8Y2aLzWyk18V4oKZzbkf+zzuBml4W45HRZrY8vynI75o6TmZmDYE4YBEBdv5POnYo5Ln3l+AX6Omc6wgMAO7Ibw4ISC6v/bLst2EWzktAE6ADsAN4ytNqipmZVQI+Bu5xzmUWfM/fz/8pjr3Q595fgn8bUK/A65j8ZQHDObct/9/pwFTymr8Cya78NtBf2kLTPa6nRDnndjnncpxzucAk/Pj8m1koecH3rnPuk/zFAXH+T3XsZ3Pu/SX4E4FmZtbIzMKAa4BpHtdUYsysYv7NHsysInARsPL0W/mdacCN+T/fCHzmYS0l7pfQy3cFfnr+zcyA14DVzrmnC7zl9+f/j479bM69X/TqAcjvwvQsEAy87px71NuKSo6ZNSbvKh8gBHjPn4/fzP4D9CVvONpdwMPAp8AHQH3yhvW+yjnnlzdA/+D4+5L3Vd8BqcCoAm3efsPMegJzgRVAbv7iB8lr6/br83+aYx9KIc+93wS/iIj4xl+aekRExEcKfhGRAKPgFxEJMAp+EZEAo+AXEQkwCn4RkQCj4BcRCTD/D5r9pZm5DSHxAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting the training and validation loss\n",
    "train_losses = [x.detach() for x in train_losses]\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9687833333333333"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction for training set\n",
    "# with torch.no_grad():\n",
    "#     output = model(x_train.cuda())\n",
    "\n",
    "with torch.no_grad():\n",
    "     output_tr = model(x_train) \n",
    "softmax_tr = torch.exp(output_tr).cpu()\n",
    "prob_tr = list(softmax_tr.numpy())\n",
    "predictions_tr = np.argmax(prob_tr, axis=1)\n",
    "\n",
    "# accuracy on training set\n",
    "accuracy_score(y_train, predictions_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9658"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generating predictions for test set\n",
    "# with torch.no_grad():\n",
    "#     output = model(x_test.cuda())\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_te = model(x_test)\n",
    "\n",
    "softmax_te = torch.exp(output_te).cpu()\n",
    "prob_te = list(softmax_te.numpy())\n",
    "predictions_te = np.argmax(prob_te, axis=1)\n",
    "\n",
    "# accuracy on training set\n",
    "accuracy_score(y_test, predictions_te)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e4876ffb0edad661dde9910715b8d0b71026c06e8e8e3a754ed29e13fca540a5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('init': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
