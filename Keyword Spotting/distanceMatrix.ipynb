{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed46ad14-d8f2-4129-a1e5-2eba96070871",
   "metadata": {},
   "source": [
    "## Distance Matrix function \n",
    "\n",
    "wenn möglich en function \"dist_mat\" z implementiere, wo als argument \"images_data\" ond \"images_keywords\" nimmt ond d distance matrix berächnet. das heisst, \"images_keywords\" esch en liste mit je eim bild vo de keywords ond s berächnet för jedes keyword en matrix vo dissimiliarities vo allne bilder usem data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da8601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5edad824-29a2-406b-8a9c-35ae3838bf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.41421356],\n",
       "       [1.41421356, 1.        ]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "distance_matrix([[0,0],[0,1]], [[1,0],[1,1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85d4290c-6339-4124-af55-2ae61245a40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1, 0, 0], [1, 0, 1], [0, 0, 1]], [[1, 1, 1], [0, 1, 0], [1, 1, 1]]]\n",
      "[[[1, 1, 1], [1, 1, 1], [0, 0, 1]], [[0, 0, 0], [0, 0, 1], [1, 0, 1]], [[1, 1, 0], [0, 1, 1], [0, 0, 1]], [[1, 1, 1], [1, 1, 1], [1, 1, 1]]]\n",
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "data = [[[1, 0, 0], [1, 0, 1], [0,0,1]], [[1,1,1], [0,1,0], [1,1,1]]]\n",
    "print(data)\n",
    "img = [[[1, 1, 1], [1, 1, 1], [0,0,1]], [[0, 0, 0], [0, 0, 1], [1,0,1]], [[1, 1, 0], [0, 1, 1], [0,0,1]], [[1, 1, 1], [1, 1, 1], [1,1,1]]]\n",
    "print(img)\n",
    "print(len(data))\n",
    "print(len(img))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e16b7c47-263c-4dec-8cee-2b77f855149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_mat(images_data, images_keywords):\n",
    "    distance_list = []\n",
    "    for keyword in images_keywords:\n",
    "        for img in images_data:\n",
    "            distance_mat = distance_matrix(keyword, img)\n",
    "            distance_list.append(distance_mat)\n",
    "    print(distance_list)\n",
    "    return distance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30ffa766-3e04-4c67-8fd6-e7855c6faf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1.41421356, 1.        , 1.41421356],\n",
      "       [1.41421356, 1.        , 1.41421356],\n",
      "       [1.41421356, 1.        , 0.        ]]), array([[0.        , 1.41421356, 0.        ],\n",
      "       [0.        , 1.41421356, 0.        ],\n",
      "       [1.41421356, 1.41421356, 1.41421356]]), array([[1.        , 1.41421356, 1.        ],\n",
      "       [1.41421356, 1.        , 0.        ],\n",
      "       [1.        , 0.        , 1.        ]]), array([[1.73205081, 1.        , 1.73205081],\n",
      "       [1.41421356, 1.41421356, 1.41421356],\n",
      "       [1.        , 1.73205081, 1.        ]]), array([[1.        , 1.41421356, 1.73205081],\n",
      "       [1.73205081, 1.41421356, 1.        ],\n",
      "       [1.41421356, 1.        , 0.        ]]), array([[1.        , 1.        , 1.        ],\n",
      "       [1.        , 1.        , 1.        ],\n",
      "       [1.41421356, 1.41421356, 1.41421356]]), array([[1.41421356, 1.        , 1.41421356],\n",
      "       [1.41421356, 1.        , 1.41421356],\n",
      "       [1.41421356, 1.        , 1.41421356]]), array([[0.        , 1.41421356, 0.        ],\n",
      "       [0.        , 1.41421356, 0.        ],\n",
      "       [0.        , 1.41421356, 0.        ]])]\n",
      "[array([[1.41421356, 1.        , 1.41421356],\n",
      "       [1.41421356, 1.        , 1.41421356],\n",
      "       [1.41421356, 1.        , 0.        ]]), array([[0.        , 1.41421356, 0.        ],\n",
      "       [0.        , 1.41421356, 0.        ],\n",
      "       [1.41421356, 1.41421356, 1.41421356]]), array([[1.        , 1.41421356, 1.        ],\n",
      "       [1.41421356, 1.        , 0.        ],\n",
      "       [1.        , 0.        , 1.        ]]), array([[1.73205081, 1.        , 1.73205081],\n",
      "       [1.41421356, 1.41421356, 1.41421356],\n",
      "       [1.        , 1.73205081, 1.        ]]), array([[1.        , 1.41421356, 1.73205081],\n",
      "       [1.73205081, 1.41421356, 1.        ],\n",
      "       [1.41421356, 1.        , 0.        ]]), array([[1.        , 1.        , 1.        ],\n",
      "       [1.        , 1.        , 1.        ],\n",
      "       [1.41421356, 1.41421356, 1.41421356]]), array([[1.41421356, 1.        , 1.41421356],\n",
      "       [1.41421356, 1.        , 1.41421356],\n",
      "       [1.41421356, 1.        , 1.41421356]]), array([[0.        , 1.41421356, 0.        ],\n",
      "       [0.        , 1.41421356, 0.        ],\n",
      "       [0.        , 1.41421356, 0.        ]])]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z7/341w3s295dg9rwzw780tkxr40000gn/T/ipykernel_70309/1705721685.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdist_mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdist_mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "dist_mat(data, img)\n",
    "\n",
    "dist_mat(data, img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e938bd2-f83a-475c-a0b9-25b2a6fbeadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the Data\n",
    "\n",
    "# For each image and each threshold (top-k matches), compute the TP, FP, FN\n",
    "# Recall = TP / (TP + FN)\n",
    "# Precision = TP / (TP + FP)\n",
    "# Average Precision (AP): Area under the Recall-Precision curve\n",
    "\n",
    "# let's get all keywords in a list\n",
    "with open(\"task/keywords.txt\", \"r\") as keywords_file:\n",
    "    keywords_list = [(line.strip()).split() for line in keywords_file]\n",
    "\n",
    "# get a list of images of the keywords (depending of the set we take the images from)\n",
    "images_keywords_in_train = [([words[k][i] for (k, i) in map_(x) if not k.startswith(\"3\")[0]]) for x in keywords_list]\n",
    "images_keywords_in_valid = [([words[k][i] for (k, i) in map_(x) if k.startswith(\"3\")[0]]) for x in keywords_list]\n",
    "\n",
    "# get a list of images form the train or valid set\n",
    "with open(\"task/train.txt\") as f:\n",
    "    train = [l.strip() for l in f]\n",
    "\n",
    "images_train_set = [words[k][i] for (k, v) in words.items() for i in v if any(t for t in train if k.startswith(t))]\n",
    "\n",
    "with open(\"task/valid.txt\") as f:\n",
    "    valid = [l.strip() for l in f]\n",
    "images_valid_set = [words[k][i] for (k, v) in words.items() for i in v if any(t for t in valid if k.startswith(t))]\n",
    "\n",
    "# calculate for the images above the distance matrices with the train or valid set\n",
    "distMat_train_keywords = dist_mat(images_train_set, images_keywords_in_valid)\n",
    "distMat_valid_keywords = dist_mat(images_valid_set, images_keywords_in_train)\n",
    "\n",
    "# now we can start the evaluation\n",
    "def evaluation(dist_train_keywords, dist_valid_keywords):\n",
    "    tp_train = np.zeros((len(images_train_set), len(keywords_list)))\n",
    "    fp_train = np.zeros((len(images_train_set), len(keywords_list)))\n",
    "    fn_train = np.zeros((len(images_train_set), len(keywords_list)))\n",
    "    precision_train = np.zeros((len(images_train_set), len(keywords_list)))\n",
    "    recall_train = np.zeros((len(images_train_set), len(keywords_list)))\n",
    "\n",
    "    tp_valid = np.zeros((len(images_valid_set), len(keywords_list)))\n",
    "    fp_valid = np.zeros((len(images_valid_set), len(keywords_list)))\n",
    "    fn_valid = np.zeros((len(images_valid_set), len(keywords_list)))\n",
    "    precision_valid = np.zeros((len(images_valid_set), len(keywords_list)))\n",
    "    recall_valid = np.zeros((len(images_valid_set), len(keywords_list)))\n",
    "\n",
    "    index = 0\n",
    "    # we start with the train set\n",
    "    for keyword in images_keywords_in_valid:\n",
    "        # all ID's of the keyword in the train set\n",
    "        keyword_ids = [(k, i) for (k, i) in map_(keyword) if k < 300]\n",
    "\n",
    "        for k in range(len(images_train_set)):\n",
    "            # get the k smallest indices\n",
    "            idx = np.argpartition(dist_train_keywords[:,index],k+1)\n",
    "            # TO DO - match the indices from the distMat with the images ID\n",
    "            kmatches_ids = [idx_func(x) for x in idx]\n",
    "            # adjust the fp, tp and fn\n",
    "            for x in kmatches_ids:\n",
    "                if x in keyword_ids:\n",
    "                    tp_train[k,index] = tp_train[k,index]+1\n",
    "                else:\n",
    "                    fp_train[k,index] = fp_train[k,index]+1\n",
    "            fn_train[k,index] = len(keyword_ids)-tp_train[k,index]\n",
    "\n",
    "            precision_train[k,index] = tp_train[k,index]/(tp_train[k,index] + fp_train[k,index])\n",
    "            recall_train[k,index] = tp_train[k,index]/(tp_train[k,index] + fn_train[k,index])\n",
    "        \n",
    "        index = index + 1\n",
    "\n",
    "    index = 0\n",
    "    # now the valid set\n",
    "    for keyword in images_keywords_in_train:\n",
    "        # all ID's of the keyword in the train set\n",
    "        keyword_ids = [(k, i) for (k, i) in map_(keyword) if k >= 300]\n",
    "\n",
    "        for k in range(len(images_valid_set)):\n",
    "            # get the k smallest indices\n",
    "            idx = np.argpartition(dist_valid_keywords[:,index],k+1)\n",
    "            # TO DO - match the indices from the distMat with the images ID\n",
    "            kmatches_ids = [idx_func(x) for x in idx]\n",
    "            # adjust the fp, tp and fn\n",
    "            for x in kmatches_ids:\n",
    "                if x in keyword_ids:\n",
    "                    tp_valid[k,index] = tp_valid[k,index]+1\n",
    "                else:\n",
    "                    fp_valid[k,index] = fp_valid[k,index]+1\n",
    "            fn_valid[k,index] = len(keyword_ids)-tp_valid[k,index]\n",
    "\n",
    "            precision_valid[k,index] = tp_valid[k,index]/(tp_valid[k,index] + fp_valid[k,index])\n",
    "            recall_valid[k,index] = tp_valid[k,index]/(tp_valid[k,index] + fn_valid[k,index])\n",
    "    \n",
    "        index = index + 1\n",
    "\n",
    "    return precision_train, recall_train, precision_valid, recall_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82201e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_t, recall_t, precision_v, recall_v = evaluation(distMat_train_keywords, distMat_valid_keywords)\n",
    "\n",
    "plt.plot(recall_t[:,0],precision_t[:,0])\n",
    "plt.plot(recall_v[:,0],precision_v[:,0])\n",
    "plt.legend(['First Image in Train', 'First Image in Validation'], loc='upper right')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,1])\n",
    "axes.set_ylim([0,1])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e5dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_t_a = [x/len(np.sum(precision_t,axis=1).tolist()) for x in np.sum(precision_t,axis=1).tolist()]\n",
    "recall_t_a = [x/len(np.sum(recall_t,axis=1).tolist()) for x in np.sum(recall_t,axis=1).tolist()]\n",
    "precision_v_a = [x/len(np.sum(precision_v,axis=1).tolist()) for x in np.sum(precision_v,axis=1).tolist()]\n",
    "recall_v_a = [x/len(np.sum(recall_v,axis=1).tolist()) for x in np.sum(recall_v,axis=1).tolist()]\n",
    "\n",
    "plt.plot(recall_t_a,precision_t_a)\n",
    "plt.plot(recall_v_a,precision_v_a)\n",
    "plt.legend(['Average Precision/Recall over all Images of Train', 'Average Precision/Recall over all Images of Valid'], loc='upper right')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,1])\n",
    "axes.set_ylim([0,1])\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
