{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41cbcf88-cb43-4bc8-8aff-d91506611ee7",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Shape Matching: Represent shapes with a string of equidistance contour points, labeled with their angle (0 if continuing in the same direction, ≠0 for turns\n",
    "\n",
    "Dynamic time warping aligns two sequences along a common time axis by considering only substitutions (xi → yj), usually with Euclidean cost: c(xi →yj)= ||xi −yj|| = ∑ (xi,k −yj,k)^1/2\n",
    "\n",
    "## Keyword Spotting: \n",
    "Describe words as feature vector sequences with a sliding window.\n",
    "Retrieve words similar to a query word from the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caba1790-57d3-4234-8fab-42275977e2fb",
   "metadata": {},
   "source": [
    "## Dynamic Time Warping \n",
    "\n",
    "In general, DTW is a method that calculates an optimal match between two given sequences (e.g. time series) with certain restriction and rules(comes from wiki). Example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11ed31e8-6f85-4646-9397-364809690203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "from dtaidistance import dtw\n",
    "s1 = [0, 0, 1, 2, 1, 0, 1, 0, 0]\n",
    "s2 = [0, 1, 2, 0, 0, 0, 0, 0, 0]\n",
    "distance = dtw.distance(s1, s2)\n",
    "print(distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4aabb4-ef98-42f3-a6be-b474fdee6e6d",
   "metadata": {},
   "source": [
    "## Our data:  \n",
    "* /ground-truth/transcription.txt:  Contains the transcription of all words (on a character level) of the whole dataset. \n",
    "* /ground-truth/locations/*.svg: Contains bounding boxes for all words in the svg-format.\n",
    "\n",
    "* /images/*.jpg: original images in jpg-format.\n",
    "\n",
    "* /task:  Contains three files: ####train.txt / valid.txt #### Contains a splitting of the documents into a training and a validation set. keywords.txt: Contains a list of keywords of which each will be at least once in the training and validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce8733-38e6-4425-869e-22a9651e0156",
   "metadata": {},
   "source": [
    "## Understanding the data \n",
    "The SVG images (in /ground-truth/locations/*.svg) contain the bounding boxes for all words. What one did was creating th polygon \"box\" aorund the separate words and the idea is that the bounding box can tell us what the word inside it is. Each word in manuscript has a single bounding box. \n",
    "\n",
    "In the transcription.txt file you can find which word would be in which bounding box eg. 270-01-03 O-r-d-e-r-s means that in document 270 on line 1 the third word is *Orders*. \n",
    "\n",
    "\n",
    "## What to do?\n",
    "The idea is to get the polygons out of the SVGs (in /ground-truth/locations/*.svg), i.e. cut out those polygons and save them in single files. \n",
    "\n",
    "Then, one has to go trough those polygons and find out which polygon is which word (this is known!). Then, re-evaluate on test set. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac804499-46a0-4d37-be6a-59cc803b7acd",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Let's start with preprocessing our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8493d3fa-a719-4acd-aefe-2eecacc35d2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ' 112.00 170.00 112.00 230.00 129.27 231.50 132.00 230.00 232.00 230.00 240.00 238.00 299.69 148.25 192.00 157.00 '",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z7/341w3s295dg9rwzw780tkxr40000gn/T/ipykernel_18416/2147000665.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m    \u001b[0;31m# print(j['d'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'M'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Z'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'  '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mpairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ' 112.00 170.00 112.00 230.00 129.27 231.50 132.00 230.00 232.00 230.00 240.00 238.00 299.69 148.25 192.00 157.00 '"
     ]
    }
   ],
   "source": [
    "#pip install svgpathtools\n",
    "#pip install svg.path\n",
    "from svg.path import parse_path\n",
    "from svgpathtools import svg2paths2, wsvg\n",
    "#Reading SVGSs: The svg2paths() function converts an svgfile to a list of Path objects and \n",
    "#a separate list of dictionaries containing the attributes of each said path\n",
    "paths, attributes, svg_attributes = svg2paths2('data/ground-truth/locations/270.svg')\n",
    "\n",
    "#print(paths)\n",
    "#print(attributes)\n",
    "#print(svg_attributes)\n",
    "pairs = []\n",
    "type(attributes)\n",
    "for j in attributes:\n",
    "   # print(j['d'])\n",
    "    string = j['d'].replace('M', '').replace('L', '').replace('Z','').replace('  ', ' ').replace('.*', '')\n",
    "    #print(int(string))\n",
    "    for i in range(0, len(string),2):\n",
    "        pairs.append([string[i], string[i+1]])\n",
    "    print(string)\n",
    "    print(pairs)\n",
    "    \n",
    "\n",
    "  \n",
    "   \n",
    "    #wsvg(paths, attributes=attributes, svg_attributes=svg_attributes, filename='outputs.svg')\n",
    "    #xmin, xmax, ymin, ymax = i.bbox()\n",
    "    #box = i.bbox()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23355ad7-e81a-438a-af39-5000361d6c35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4997a32f-2a0b-49b3-a6db-71a3c41c1031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 340\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588f571-10b6-4e04-b409-5f484071e8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cdd210-2b1a-4570-924e-928594693e86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
